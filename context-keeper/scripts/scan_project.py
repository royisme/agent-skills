#!/usr/bin/env python3
"""
Project Scanner for context-keeper

Scans a project directory to:
1. Detect tech stack from config files (package.json, go.mod, pyproject.toml, etc.)
2. Infer coding conventions based on tech stack
3. Generate USERAGENTS.md with project structure and coding rules
4. Create TECH_INFO.md templates for each directory
5. Update AGENTS.md/CLAUDE.md to enforce reading USERAGENTS.md

Usage:
    python scan_project.py <project-path> [--dry-run]
"""

import argparse
import json
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Optional


# ============================================================================
# Tech Stack Detection
# ============================================================================

TECH_STACK_DETECTORS = {
    "typescript": {
        "files": ["package.json", "tsconfig.json"],
        "check": lambda p: (p / "tsconfig.json").exists() or _has_ts_dep(p),
    },
    "javascript": {
        "files": ["package.json"],
        "check": lambda p: (p / "package.json").exists() and not (p / "tsconfig.json").exists(),
    },
    "react": {
        "files": ["package.json"],
        "check": lambda p: _has_dep(p, "react"),
    },
    "vue": {
        "files": ["package.json"],
        "check": lambda p: _has_dep(p, "vue"),
    },
    "astro": {
        "files": ["package.json", "astro.config.mjs"],
        "check": lambda p: _has_dep(p, "astro"),
    },
    "nextjs": {
        "files": ["package.json", "next.config.js", "next.config.mjs"],
        "check": lambda p: _has_dep(p, "next"),
    },
    "go": {
        "files": ["go.mod"],
        "check": lambda p: (p / "go.mod").exists(),
    },
    "python": {
        "files": ["pyproject.toml", "requirements.txt", "setup.py"],
        "check": lambda p: any((p / f).exists() for f in ["pyproject.toml", "requirements.txt", "setup.py"]),
    },
    "rust": {
        "files": ["Cargo.toml"],
        "check": lambda p: (p / "Cargo.toml").exists(),
    },
    "java": {
        "files": ["pom.xml", "build.gradle"],
        "check": lambda p: (p / "pom.xml").exists() or (p / "build.gradle").exists(),
    },
}


def _read_package_json(project_path: Path) -> dict:
    """Read package.json if exists."""
    pkg_path = project_path / "package.json"
    if pkg_path.exists():
        try:
            return json.loads(pkg_path.read_text())
        except json.JSONDecodeError:
            return {}
    return {}


def _has_dep(project_path: Path, dep_name: str) -> bool:
    """Check if a dependency exists in package.json."""
    pkg = _read_package_json(project_path)
    deps = pkg.get("dependencies", {})
    dev_deps = pkg.get("devDependencies", {})
    return dep_name in deps or dep_name in dev_deps


def _has_ts_dep(project_path: Path) -> bool:
    """Check if TypeScript is a dependency."""
    return _has_dep(project_path, "typescript")


def detect_tech_stack(project_path: Path) -> list[str]:
    """Detect all tech stacks used in the project."""
    detected = []
    for tech, config in TECH_STACK_DETECTORS.items():
        try:
            if config["check"](project_path):
                detected.append(tech)
        except Exception:
            pass
    return detected


# ============================================================================
# Coding Conventions Inference
# ============================================================================

CODING_CONVENTIONS = {
    "typescript": [
        "ç¦æ­¢ä½¿ç”¨ `any` ç±»å‹ï¼Œå¿…é¡»ä½¿ç”¨æ˜ç¡®çš„ç±»å‹å®šä¹‰",
        "ä½¿ç”¨ `unknown` ä»£æ›¿ `any` å¤„ç†æœªçŸ¥ç±»å‹",
        "æ‰€æœ‰å‡½æ•°å¿…é¡»æœ‰æ˜ç¡®çš„è¿”å›ç±»å‹å£°æ˜",
        "ä½¿ç”¨ `interface` å®šä¹‰å¯¹è±¡ç»“æ„ï¼Œ`type` å®šä¹‰è”åˆç±»å‹æˆ–å¤æ‚ç±»å‹",
        "å¯ç”¨ strict æ¨¡å¼ä¸‹çš„æ‰€æœ‰æ£€æŸ¥",
    ],
    "javascript": [
        "ä½¿ç”¨ ES6+ è¯­æ³•",
        "ä½¿ç”¨ const å’Œ letï¼Œç¦æ­¢ var",
        "ä½¿ç”¨è§£æ„èµ‹å€¼ç®€åŒ–ä»£ç ",
    ],
    "react": [
        "ä½¿ç”¨å‡½æ•°ç»„ä»¶å’Œ Hooksï¼Œé¿å… class ç»„ä»¶",
        "ç»„ä»¶æ–‡ä»¶ä½¿ç”¨ PascalCase å‘½å",
        "ä½¿ç”¨ React.memo() ä¼˜åŒ–æ¸²æŸ“æ€§èƒ½",
        "ä½¿ç”¨ useMemo/useCallback é¿å…ä¸å¿…è¦çš„é‡æ¸²æŸ“",
    ],
    "astro": [
        "ä½¿ç”¨ Astro ç»„ä»¶å¤„ç†é™æ€å†…å®¹",
        "ä»…åœ¨éœ€è¦äº¤äº’æ—¶ä½¿ç”¨ React/Vue å²›å±¿ç»„ä»¶",
        "éµå¾ª Astro çš„æ–‡ä»¶è·¯ç”±çº¦å®š",
    ],
    "nextjs": [
        "ä½¿ç”¨ App Router (app/) è€Œé Pages Router",
        "ä½¿ç”¨ Server Components ä½œä¸ºé»˜è®¤",
        "ä»…åœ¨éœ€è¦äº¤äº’æ—¶ä½¿ç”¨ 'use client'",
    ],
    "go": [
        "éµå¾ª Go å®˜æ–¹ä»£ç è§„èŒƒ (Effective Go)",
        "ä½¿ç”¨ gofmt æ ¼å¼åŒ–ä»£ç ",
        "é”™è¯¯å¿…é¡»æ˜¾å¼å¤„ç†ï¼Œç¦æ­¢å¿½ç•¥ error è¿”å›å€¼",
        "ä½¿ç”¨æœ‰æ„ä¹‰çš„å˜é‡åï¼Œé¿å…å•å­—æ¯å˜é‡ï¼ˆå¾ªç¯å˜é‡é™¤å¤–ï¼‰",
    ],
    "python": [
        "éµå¾ª PEP 8 ä»£ç è§„èŒƒ",
        "ä½¿ç”¨ç±»å‹æç¤º (Type Hints)",
        "ä½¿ç”¨ f-string è¿›è¡Œå­—ç¬¦ä¸²æ ¼å¼åŒ–",
        "ä½¿ç”¨ pathlib è€Œé os.path",
    ],
    "rust": [
        "ä½¿ç”¨ cargo fmt æ ¼å¼åŒ–ä»£ç ",
        "ä½¿ç”¨ cargo clippy è¿›è¡Œä»£ç æ£€æŸ¥",
        "ä¼˜å…ˆä½¿ç”¨ Result è€Œé panic",
        "æ‰€æœ‰å…¬å…± API å¿…é¡»æœ‰æ–‡æ¡£æ³¨é‡Š",
    ],
    "common": [
        "ç¦æ­¢ç›´æ¥ä½¿ç”¨åŸç”Ÿ fetchï¼Œå¿…é¡»é€šè¿‡å°è£…çš„ HTTP å·¥å…·ç±»å‘èµ·è¯·æ±‚",
        "ç¦æ­¢ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯ï¼ˆAPI keysã€å¯†ç ç­‰ï¼‰",
        "ç¦æ­¢æäº¤ .env ç­‰é…ç½®æ–‡ä»¶åˆ° git",
        "æ‰€æœ‰å¼‚æ­¥æ“ä½œå¿…é¡»æœ‰é€‚å½“çš„é”™è¯¯å¤„ç†",
    ],
}


def get_coding_conventions(tech_stacks: list[str]) -> list[str]:
    """Get coding conventions based on detected tech stacks."""
    conventions = []

    # Add common conventions first
    conventions.extend(CODING_CONVENTIONS.get("common", []))

    # Add tech-specific conventions
    for tech in tech_stacks:
        if tech in CODING_CONVENTIONS:
            conventions.extend(CODING_CONVENTIONS[tech])

    return conventions


# ============================================================================
# Directory Structure Analysis
# ============================================================================

# Default directories to always ignore
DEFAULT_IGNORE_DIRS = {
    # Version control
    ".git", ".svn",
    # IDE & editors
    ".idea", ".vscode", ".cursor",
}


def parse_gitignore(project_path: Path) -> set[str]:
    """Parse .gitignore and extract directory patterns to ignore."""
    gitignore_path = project_path / ".gitignore"
    ignore_dirs = set()

    if not gitignore_path.exists():
        return ignore_dirs

    try:
        content = gitignore_path.read_text()
        for line in content.splitlines():
            line = line.strip()
            # Skip comments and empty lines
            if not line or line.startswith("#"):
                continue
            # Skip negation patterns
            if line.startswith("!"):
                continue
            # Handle directory patterns (ending with / or just directory names)
            if line.endswith("/"):
                ignore_dirs.add(line.rstrip("/"))
            else:
                # Also treat non-path patterns as potential directory names
                # Only if they don't contain wildcards or path separators
                if "*" not in line and "?" not in line:
                    # Remove leading slash if present
                    clean_line = line.lstrip("/")
                    if "/" not in clean_line:
                        ignore_dirs.add(clean_line)
    except Exception:
        pass

    return ignore_dirs


def get_ignore_dirs(project_path: Path) -> set[str]:
    """Get combined set of directories to ignore."""
    # Start with default ignores
    ignore_dirs = DEFAULT_IGNORE_DIRS.copy()

    # Add patterns from .gitignore
    gitignore_patterns = parse_gitignore(project_path)
    ignore_dirs.update(gitignore_patterns)

    return ignore_dirs


def analyze_directory_structure(project_path: Path, max_depth: int = 3) -> dict:
    """Analyze project directory structure."""
    structure = {}

    # Get ignore patterns from .gitignore
    ignore_dirs = get_ignore_dirs(project_path)

    def scan_dir(path: Path, current_depth: int = 0) -> Optional[dict]:
        if current_depth > max_depth:
            return None
        if path.name in ignore_dirs:
            return None
        if not path.is_dir():
            return None

        result = {
            "name": path.name,
            "path": str(path.relative_to(project_path)),
            "files": [],
            "subdirs": [],
        }

        try:
            for item in sorted(path.iterdir()):
                if item.is_file() and not item.name.startswith("."):
                    result["files"].append(item.name)
                elif item.is_dir() and item.name not in ignore_dirs:
                    subdir = scan_dir(item, current_depth + 1)
                    if subdir:
                        result["subdirs"].append(subdir)
        except PermissionError:
            pass

        return result

    return scan_dir(project_path, 0)


def infer_directory_purpose(dir_name: str, files: list[str]) -> str:
    """Infer the purpose of a directory based on its name and contents."""
    name_lower = dir_name.lower()

    purpose_map = {
        "src": "æºä»£ç ä¸»ç›®å½•",
        "lib": "åº“æ–‡ä»¶å’Œå·¥å…·å‡½æ•°",
        "utils": "é€šç”¨å·¥å…·å‡½æ•°",
        "helpers": "è¾…åŠ©å‡½æ•°",
        "components": "UI ç»„ä»¶",
        "pages": "é¡µé¢ç»„ä»¶/è·¯ç”±",
        "app": "åº”ç”¨æ ¸å¿ƒé€»è¾‘",
        "api": "API æ¥å£å®šä¹‰",
        "services": "ä¸šåŠ¡æœåŠ¡å±‚",
        "hooks": "React Hooks",
        "stores": "çŠ¶æ€ç®¡ç†",
        "store": "çŠ¶æ€ç®¡ç†",
        "types": "ç±»å‹å®šä¹‰",
        "interfaces": "æ¥å£å®šä¹‰",
        "models": "æ•°æ®æ¨¡å‹",
        "schemas": "æ•°æ®æ ¡éªŒ schema",
        "config": "é…ç½®æ–‡ä»¶",
        "constants": "å¸¸é‡å®šä¹‰",
        "assets": "é™æ€èµ„æº",
        "public": "å…¬å…±é™æ€æ–‡ä»¶",
        "static": "é™æ€æ–‡ä»¶",
        "styles": "æ ·å¼æ–‡ä»¶",
        "css": "CSS æ ·å¼",
        "tests": "æµ‹è¯•æ–‡ä»¶",
        "test": "æµ‹è¯•æ–‡ä»¶",
        "__tests__": "æµ‹è¯•æ–‡ä»¶",
        "spec": "æµ‹è¯•è§„èŒƒ",
        "scripts": "è„šæœ¬æ–‡ä»¶",
        "bin": "å¯æ‰§è¡Œæ–‡ä»¶",
        "docs": "æ–‡æ¡£",
        "migrations": "æ•°æ®åº“è¿ç§»",
        "middleware": "ä¸­é—´ä»¶",
        "plugins": "æ’ä»¶",
        "layouts": "å¸ƒå±€ç»„ä»¶",
        "templates": "æ¨¡æ¿æ–‡ä»¶",
        "features": "åŠŸèƒ½æ¨¡å—",
        "modules": "ä¸šåŠ¡æ¨¡å—",
        "domain": "é¢†åŸŸæ¨¡å‹",
        "infrastructure": "åŸºç¡€è®¾æ–½å±‚",
        "adapters": "é€‚é…å™¨å±‚",
        "ports": "ç«¯å£å®šä¹‰",
    }

    for key, purpose in purpose_map.items():
        if name_lower == key or name_lower.endswith(key):
            return purpose

    return "[å¾…è¡¥å……ï¼šè¯·æè¿°è¯¥ç›®å½•çš„åŠŸèƒ½]"


# ============================================================================
# File Generation
# ============================================================================

def generate_useragents_md(
    project_path: Path,
    tech_stacks: list[str],
    conventions: list[str],
    structure: dict,
) -> str:
    """Generate USERAGENTS.md content."""

    project_name = project_path.name
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    content = f"""# {project_name} - é¡¹ç›®ä¸Šä¸‹æ–‡å¼•å¯¼

> **è‡ªåŠ¨ç”Ÿæˆæ—¶é—´**: {timestamp}
> **æŠ€æœ¯æ ˆ**: {', '.join(tech_stacks) if tech_stacks else 'æœªæ£€æµ‹åˆ°'}

---

## âš ï¸ å¼ºåˆ¶æ‰§è¡Œè§„åˆ™

**æ¯æ¬¡æ“ä½œå‰ï¼Œå¿…é¡»æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š**

1. **é˜…è¯»ç›¸å…³ç›®å½•çš„ TECH_INFO.md** - äº†è§£è¯¥ç›®å½•ä¸‹å„æ–‡ä»¶çš„åŠŸèƒ½å’Œä¾èµ–å…³ç³»
2. **éµå¾ªä¸‹æ–¹çš„ç¼–ç è§„èŒƒ** - ç¡®ä¿ä»£ç ç¬¦åˆé¡¹ç›®æ ‡å‡†
3. **ä¿®æ”¹å®Œæˆåæ›´æ–°æ–‡æ¡£** - åŒæ­¥æ›´æ–° TECH_INFO.md å’Œæ–‡ä»¶å¤´æ³¨é‡Š

---

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„

"""

    def render_structure(node: dict, indent: int = 0) -> str:
        lines = []
        prefix = "  " * indent
        purpose = infer_directory_purpose(node["name"], node["files"])

        if indent == 0:
            lines.append(f"```")
            lines.append(f"{node['name']}/")
        else:
            lines.append(f"{prefix}â”œâ”€â”€ {node['name']}/  # {purpose}")
            lines.append(f"{prefix}â”‚   â””â”€â”€ TECH_INFO.md  # ğŸ“„ ç›®å½•æŠ€æœ¯æ–‡æ¡£")

        for subdir in node.get("subdirs", []):
            lines.append(render_structure(subdir, indent + 1))

        if indent == 0:
            lines.append(f"â””â”€â”€ USERAGENTS.md  # ğŸ“Œ æœ¬å¼•å¯¼æ–‡ä»¶")
            lines.append(f"```")

        return "\n".join(lines)

    if structure:
        content += render_structure(structure)

    content += f"""

---

## ğŸ“‹ ç¼–ç è§„èŒƒ

ä»¥ä¸‹è§„èŒƒå¿…é¡»ä¸¥æ ¼éµå®ˆï¼š

"""

    for i, conv in enumerate(conventions, 1):
        content += f"{i}. {conv}\n"

    content += """

---

## ğŸ“ æ–‡æ¡£ç»´æŠ¤è§„åˆ™

### TECH_INFO.md ç»´æŠ¤

æ¯ä¸ªç›®å½•å¿…é¡»åŒ…å« `TECH_INFO.md` æ–‡ä»¶ï¼Œå†…å®¹åŒ…æ‹¬ï¼š

```markdown
# [ç›®å½•å] æŠ€æœ¯æ–‡æ¡£

## æ–‡ä»¶æ¸…å•

| æ–‡ä»¶å | åŠŸèƒ½æè¿° | å…¥å‚ | å‡ºå‚ | ä¾èµ– |
|--------|----------|------|------|------|
| xxx.ts | æè¿°åŠŸèƒ½ | ç±»å‹ | ç±»å‹ | ä¾èµ–æ–‡ä»¶ |

## æœ€è¿‘å˜æ›´

- [æ—¥æœŸ] [å˜æ›´å†…å®¹]
```

### æ–‡ä»¶å¤´æ³¨é‡Šè§„èŒƒ

æ¯ä¸ªä»£ç æ–‡ä»¶å¿…é¡»åŒ…å«å¤´éƒ¨æ³¨é‡Šï¼š

```typescript
/**
 * @file æ–‡ä»¶å
 * @description åŠŸèƒ½æè¿°
 * @module æ‰€å±æ¨¡å—
 * @dependencies ä¾èµ–çš„å…¶ä»–æ–‡ä»¶
 * @lastModified YYYY-MM-DD
 */
```

### å¼ºåˆ¶æ›´æ–°æ—¶æœº

åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼Œ**å¿…é¡»**æ›´æ–°ç›¸å…³æ–‡æ¡£ï¼š

1. âœ… æ–°å¢æ–‡ä»¶ â†’ æ›´æ–° TECH_INFO.md æ–‡ä»¶æ¸…å•
2. âœ… ä¿®æ”¹æ–‡ä»¶åŠŸèƒ½ â†’ æ›´æ–°æ–‡ä»¶å¤´æ³¨é‡Šå’Œ TECH_INFO.md
3. âœ… åˆ é™¤æ–‡ä»¶ â†’ ä» TECH_INFO.md ç§»é™¤
4. âœ… ä¿®æ”¹ä¾èµ–å…³ç³» â†’ æ›´æ–°ä¾èµ–è¯´æ˜
5. âœ… æ–°å¢ç›®å½• â†’ åˆ›å»ºæ–°çš„ TECH_INFO.md

---

## ğŸ”— ç›®å½•æ–‡æ¡£ç´¢å¼•

"""

    def list_tech_info_links(node: dict, base_path: str = "") -> list[str]:
        links = []
        current_path = f"{base_path}/{node['name']}" if base_path else node['name']

        if base_path:  # Skip root
            purpose = infer_directory_purpose(node["name"], node["files"])
            links.append(f"- [{node['name']}]({current_path}/TECH_INFO.md) - {purpose}")

        for subdir in node.get("subdirs", []):
            links.extend(list_tech_info_links(subdir, current_path))

        return links

    if structure:
        for link in list_tech_info_links(structure):
            content += link + "\n"

    return content


def generate_tech_info_md(dir_name: str, files: list[str]) -> str:
    """Generate TECH_INFO.md template for a directory."""

    purpose = infer_directory_purpose(dir_name, files)
    timestamp = datetime.now().strftime("%Y-%m-%d")

    content = f"""# {dir_name} - æŠ€æœ¯æ–‡æ¡£

> **ç›®å½•åŠŸèƒ½**: {purpose}
> **æœ€åæ›´æ–°**: {timestamp}

---

## ğŸ“ æ–‡ä»¶æ¸…å•

| æ–‡ä»¶å | åŠŸèƒ½æè¿° | å…¥å‚ | å‡ºå‚ | ä¾èµ– |
|--------|----------|------|------|------|
"""

    for file in sorted(files):
        if file.endswith(('.ts', '.tsx', '.js', '.jsx', '.py', '.go', '.rs')):
            content += f"| `{file}` | [å¾…è¡¥å……] | [å¾…è¡¥å……] | [å¾…è¡¥å……] | [å¾…è¡¥å……] |\n"

    if not any(f.endswith(('.ts', '.tsx', '.js', '.jsx', '.py', '.go', '.rs')) for f in files):
        content += "| (æ— ä»£ç æ–‡ä»¶) | - | - | - | - |\n"

    content += """

---

## ğŸ”„ å˜æ›´è®°å½•

| æ—¥æœŸ | å˜æ›´å†…å®¹ | æ“ä½œè€… |
|------|----------|--------|
| {timestamp} | åˆå§‹åŒ–æŠ€æœ¯æ–‡æ¡£ | context-keeper |

---

## ğŸ“ å¤‡æ³¨

[åœ¨æ­¤æ·»åŠ è¯¥ç›®å½•çš„ç‰¹æ®Šè¯´æ˜ã€æ¶æ„å†³ç­–æˆ–æ³¨æ„äº‹é¡¹]
""".format(timestamp=timestamp)

    return content


def generate_agents_patch(project_path: Path) -> str:
    """Generate content to add to AGENTS.md/CLAUDE.md."""

    return """
## ğŸ”’ å¼ºåˆ¶æ‰§è¡Œï¼šä¸Šä¸‹æ–‡ç»´æŠ¤ (context-keeper)

**åœ¨å¼€å§‹ä»»ä½•ä»£ç ä¿®æ”¹å‰ï¼Œå¿…é¡»æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š**

1. **é˜…è¯» USERAGENTS.md** - äº†è§£é¡¹ç›®ç»“æ„å’Œç¼–ç è§„èŒƒ
2. **é˜…è¯»ç›®æ ‡ç›®å½•çš„ TECH_INFO.md** - äº†è§£è¯¥ç›®å½•ä¸‹å„æ–‡ä»¶çš„åŠŸèƒ½
3. **éµå¾ªç¼–ç è§„èŒƒ** - ç¡®ä¿ä»£ç ç¬¦åˆé¡¹ç›®æ ‡å‡†

**å®Œæˆä»£ç ä¿®æ”¹åï¼Œå¿…é¡»æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š**

1. **æ›´æ–° TECH_INFO.md** - å¦‚æœæ–°å¢/ä¿®æ”¹/åˆ é™¤äº†æ–‡ä»¶
2. **æ›´æ–°æ–‡ä»¶å¤´æ³¨é‡Š** - ç¡®ä¿ @description å’Œ @lastModified å‡†ç¡®
3. **æ£€æŸ¥ USERAGENTS.md** - å¦‚æœé¡¹ç›®ç»“æ„å‘ç”Ÿå˜åŒ–åˆ™æ›´æ–°

**è¿™æ˜¯å¼ºåˆ¶è¦æ±‚ï¼Œä¸å¯è·³è¿‡ã€‚**

"""


# ============================================================================
# Main Execution
# ============================================================================

def create_tech_info_files(project_path: Path, structure: dict, dry_run: bool = False):
    """Create TECH_INFO.md files for each directory."""

    def process_dir(node: dict, parent_path: Path):
        if node["path"] == ".":
            dir_path = parent_path
        else:
            dir_path = parent_path / node["path"]

        # Skip root directory
        if node["path"] != ".":
            tech_info_path = dir_path / "TECH_INFO.md"
            content = generate_tech_info_md(node["name"], node["files"])

            if dry_run:
                print(f"[DRY-RUN] Would create: {tech_info_path}")
            else:
                tech_info_path.write_text(content)
                print(f"âœ… Created: {tech_info_path}")

        for subdir in node.get("subdirs", []):
            process_dir(subdir, parent_path)

    if structure:
        process_dir(structure, project_path)


def update_agents_file(project_path: Path, dry_run: bool = False) -> bool:
    """Update AGENTS.md or CLAUDE.md with context-keeper instructions."""

    agents_files = ["AGENTS.md", "CLAUDE.md"]
    patch_content = generate_agents_patch(project_path)
    marker = "## ğŸ”’ å¼ºåˆ¶æ‰§è¡Œï¼šä¸Šä¸‹æ–‡ç»´æŠ¤ (context-keeper)"

    for filename in agents_files:
        filepath = project_path / filename
        if filepath.exists():
            current_content = filepath.read_text()

            # Check if already patched
            if marker in current_content:
                print(f"â„¹ï¸  {filename} already contains context-keeper instructions")
                continue

            # Add patch at the beginning after any frontmatter
            lines = current_content.split("\n")
            insert_idx = 0

            # Skip YAML frontmatter if present
            if lines and lines[0].strip() == "---":
                for i, line in enumerate(lines[1:], 1):
                    if line.strip() == "---":
                        insert_idx = i + 1
                        break

            new_content = "\n".join(lines[:insert_idx]) + "\n" + patch_content + "\n".join(lines[insert_idx:])

            if dry_run:
                print(f"[DRY-RUN] Would update: {filepath}")
            else:
                filepath.write_text(new_content)
                print(f"âœ… Updated: {filepath}")

            return True

    # No existing file found, create AGENTS.md
    filepath = project_path / "AGENTS.md"
    content = f"# Agent Instructions\n\n{patch_content}"

    if dry_run:
        print(f"[DRY-RUN] Would create: {filepath}")
    else:
        filepath.write_text(content)
        print(f"âœ… Created: {filepath}")

    return True


def main():
    parser = argparse.ArgumentParser(
        description="Scan project and generate context documentation"
    )
    parser.add_argument("project_path", help="Path to the project directory")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be done without making changes")

    args = parser.parse_args()
    project_path = Path(args.project_path).resolve()

    if not project_path.exists():
        print(f"âŒ Error: Project path does not exist: {project_path}")
        sys.exit(1)

    if not project_path.is_dir():
        print(f"âŒ Error: Project path is not a directory: {project_path}")
        sys.exit(1)

    print(f"ğŸ” Scanning project: {project_path}")
    print()

    # Step 1: Detect tech stack
    print("ğŸ“Š Detecting tech stack...")
    tech_stacks = detect_tech_stack(project_path)
    print(f"   Found: {', '.join(tech_stacks) if tech_stacks else 'None detected'}")
    print()

    # Step 2: Get coding conventions
    print("ğŸ“‹ Inferring coding conventions...")
    conventions = get_coding_conventions(tech_stacks)
    print(f"   {len(conventions)} rules generated")
    print()

    # Step 3: Analyze directory structure
    print("ğŸ“ Analyzing directory structure...")
    structure = analyze_directory_structure(project_path)
    print()

    # Step 4: Generate USERAGENTS.md
    print("ğŸ“ Generating USERAGENTS.md...")
    useragents_content = generate_useragents_md(project_path, tech_stacks, conventions, structure)
    useragents_path = project_path / "USERAGENTS.md"

    if args.dry_run:
        print(f"[DRY-RUN] Would create: {useragents_path}")
    else:
        useragents_path.write_text(useragents_content)
        print(f"âœ… Created: {useragents_path}")
    print()

    # Step 5: Create TECH_INFO.md files
    print("ğŸ“„ Creating TECH_INFO.md files...")
    create_tech_info_files(project_path, structure, args.dry_run)
    print()

    # Step 6: Update AGENTS.md/CLAUDE.md
    print("ğŸ”§ Updating agent configuration...")
    update_agents_file(project_path, args.dry_run)
    print()

    # Step 7: Update .gitignore
    gitignore_path = project_path / ".gitignore"
    tech_info_pattern = "TECH_INFO.md"

    if gitignore_path.exists():
        gitignore_content = gitignore_path.read_text()
        if tech_info_pattern not in gitignore_content:
            if args.dry_run:
                print(f"[DRY-RUN] Would add TECH_INFO.md to .gitignore")
            else:
                with open(gitignore_path, "a") as f:
                    f.write(f"\n# context-keeper\n{tech_info_pattern}\n")
                print(f"âœ… Added TECH_INFO.md to .gitignore")

    print()
    print("ğŸ‰ Done! Project context documentation generated successfully.")
    print()
    print("Next steps:")
    print("1. Review and customize USERAGENTS.md")
    print("2. Fill in [å¾…è¡¥å……] sections in TECH_INFO.md files")
    print("3. The agent will now maintain these documents automatically")


if __name__ == "__main__":
    main()
